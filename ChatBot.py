# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CK6HxTFhcdBtr1mlOu-odMhLkfhoBn3g
"""

#!pip install tflearn
#i am facing problem in installing nlkt(wordnet, punkt) on my device but the code 
#runs[atleast it was running before the weight adjustment in my output so as to get a single output instead of whole data]

# Imports
import nltk

nltk.download('wordnet')
nltk.download('punkt')
from nltk.stem.lancaster import LancasterStemmer
import numpy as np
import tflearn
import tensorflow as tf             
import random
import json
import pickle

#Loading Data
# with open("Intents.json") as file:        
#   data = json.load(file)
# # Initializing empty lists


# f=open('Intents.json')
data={"intents": [
  {
  "tag": "Greetings",
  "patterns":["Hi","Yo","Hello"],
  "responses":["Geetings to you too","Hi there!, how can i help?"],
   "context_set": "",

  },

  {
    "tag": "About the website",
    "patterns": ["What services do we provide", "List of best colleges", "preference order", "Jossa counselling"],
    "responses": [ "Brahmastra","Prachand"],
    "context_set": "",
   },

  
  {"tag": "goodbye",
  "patterns": ["cya", "See you later", "Goodbye", "I am Leaving", "Have a Good day", "cya later", "I gotta go now", "I gotta rush now"],
  "responses": ["Sad to see you go :(", "Talk to you later", "Goodbye!"],
  "context_set": "",
  "weight": [2, 1, 1]  
  
  },
  
  {"tag": "age",
  "patterns": ["how old", "how old is Dexter", "what is your age", "how old are you", "age?"],
  "responses": ["My master built me just a month ago.", "Just a month old!","I won't tell you"],
  "context_set": "",

  },
  {"tag": "name",
  "patterns": ["what is your name", "what should I call you", "whats your name?"],
  "responses": ["You can call me Dexter.", "I'm Dexter!", "I'm Dexter aka The Avricus Superbot."],
  "context_set": "",
 
  },
  
  {"tag": "contact",
  "patterns": ["contact information", "contact us", "how can i contact you", "can i get the contact details", "I wanna give some feedback", "how can i give some feedback?"],
  "responses": ["You can contact us at contact@avricus.com"],
  "context_set": "",
  
  },
  {"tag": "address",
  "patterns": ["what is the location?","whats the location", "where are you locatated?", "where is the company located?", "address", "whats the address?"],
  "responses": ["You can locate us at Aggarwal corporates 1208, pitam pura New Delhi"],
  "context_set": "",
  
}
  ]


}

words = []
labels = []
docs_x = []
docs_y = []

# Looping through our data, loop
for intent in data['intents']:
    for pattern in intent['patterns']:
        pattern = pattern.lower()
        # Creating a list of words
        wrds = nltk.word_tokenize(pattern)
        words.extend(wrds)
        docs_x.append(wrds)
        docs_y.append(intent['tag'])
        
        # As we loop through the data, we convert all patterns into lowercase,
        # tokenize each pattern, and then add them to the respective lists.
        # words: Holds a list of unique words.
        # labels: Holds a list of all the unique tags in the file.
        # docs_x: Holds a list of patterns.
        # docs_y: Holds a list of tags corresponding to the pattern in docs_x.

    if intent['tag'] not in labels:
        labels.append(intent['tag'])


        
stemmer = LancasterStemmer()
words = [stemmer.stem(w.lower()) for w in words if w not in "?"]
words = sorted(list(set(words)))
labels = sorted(labels)

training = []
output = []

out_empty = [0 for _ in range(len(labels))]
for x,doc in enumerate(docs_x):
  bag = []
  wrds = [stemmer.stem(w) for w in doc]
  for w in words:
    if w in wrds:
      bag.append(1)
    else:
      bag.append(0)
  output_row = out_empty[:]
  output_row[labels.index(docs_y[x])] = 1
  training.append(bag)
  output.append(output_row)
#Converting training data into NumPy arrays
training = np.array(training)
output = np.array(output)

#Saving data to disk
with open("data.pickle","wb") as f:
  pickle.dump((words, labels, training, output),f)
    
    
#building the model
 #remove train.py, exclude it 

net = tflearn.input_data(shape = [None, len(training[0])])
net = tflearn.fully_connected(net,8)
net = tflearn.fully_connected(net,8)
net = tflearn.fully_connected(net,len(labels), activation = "softmax")
net = tflearn.regression(net)
print(len(output))
model = tflearn.DNN(net)

model.fit(training, output, n_epoch = 500, batch_size = 8, show_metric = True)
model.save("model.tflearn")
    
    
#Now the below code considers the interaction of the model with user
# Load the saved model
model.load("model.tflearn")

# Load the saved data
with open("data.pickle", "rb") as f:
    words, labels, training, output = pickle.load(f)

def preprocess_input(sentence):
    # Tokenize the input sentence
    sentence_words = nltk.word_tokenize(sentence)
    # Stem the words in the input sentence
    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]
    return sentence_words

def generate_bag_of_words(sentence, words):
    # Preprocess the input sentence
    sentence_words = preprocess_input(sentence)
    # Create a bag of words representation for the input
    bag = [0] * len(words)
    for w in sentence_words:
        if w in words:
            bag[words.index(w)] = 1
    return np.array(bag)

import random
def classify_and_get_response(sentence):
    # Generate a bag of words representation for the input
    bag_of_words = generate_bag_of_words(sentence, words)
    # Make a prediction using the trained model
    results = model.predict([bag_of_words])[0]
    # Get the index of the most probable tag
    index = np.argmax(results)
    tag = labels[index]
    # Select responses from the tag's responses
    intent = next((intent for intent in data['intents'] if intent['tag'] == tag), None)
    if intent:
        responses = intent['responses']
        weights = intent.get('weight', [1] * len(responses))
        # Select a response based on weights
        response = random.choices(responses, weights=weights)[0]
        return response
    else:
        return None




